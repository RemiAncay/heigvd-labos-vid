---
title: "VID"
subtitle: "Modèles de régression linéaires (TP 2)"
author:
  - name: Rémi Ançay & Lucas Charbonnier
highlight-style: github
format:
  html:
    theme: cosmo
    monobackgroundcolor: rgb(255,250,240)
    toc: true
    toc-location: left
    #reference-location: margin
    reference-location: document
    code-line-numbers: true
date: 'last-modified'
date-format: '[This version:] MMMM D, YYYY'
number-sections: false
editor: 
  visual
execute:
  echo: true
  message: true
  warning: false
  output: true
---

## Introduction

Dans ce deuxième TP, nous allons nous préciser sur les modèles de régression linéaire. Pour se faire, nous allons étudier plusieurs jeux de données et resortir leurs caractéristiques.

### Gestion des librairies
```{r}
#| output: false

download = TRUE

r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

if(download) {
  install.packages("ggResidpanel")
  install.packages("tidyverse")
  install.packages("ggrepel")
  install.packages("GGally")
  install.packages("rgl")
  install.packages("scatterplot3d")
  install.packages("readxl")
  install.packages("leaps")
  install.packages("performance")
}

library("ggResidpanel")
library("tidyverse")
library("ggrepel")
library("GGally")
library("rgl")
library("scatterplot3d")
library("readxl")
library("leaps")
library("performance")


```

## Exercice 1

### Chargement des données
```{r}
# Chargement des données
kalama <- read.table("data/kalama.txt", header=TRUE)
```

### Coefficient de correlation

```{r}
# Coefficient de corelation entre la taille et l'âge (0.994)
cor(kalama)
```

### Nuage de points

```{r}
# Nuage de points du dataset
plot(kalama)
```

### Régression linéaire

```{r}
# Regression linéaire
kalama.lm <- lm(taille~age, data=kalama)
summary(kalama.lm)
coef(kalama.lm)
# Pente (beta 0) : 0.635
# Ordonnée à l'origine (beta 1) : 64.9283
```

### Ajustement du graphique

```{r}
plot(kalama)
abline(kalama.lm)
```

### Variance des résidus et coefficient $R^2$

```{r}
# On peut obtenir la variance des résidus avec la fonction deviance() -> 0.655
deviance(kalama.lm)

# Le coefficient de détermination R^2 est égal au carré du coefficient de correlation et est affiché dans le summary() de la régression.
# R^2 = 0.9888
```

### Diagnostic du modèle

```{r}
resid_panel(kalama.lm, plots="all")
```

Grâce au Q-Q Plot et à la Cook's Distance, on peut voir qu'il y a deux valeurs en début de série qui sorte du lot. Le reste des valeurs suivent une corrélation assez nette.

### Évaluation de l'ajustement du modèle
La droite de régression est très pertinente et décrit bien la corrélation entre les données. De plus, le coefficient de détermination $R^2$ est très proche de 1 (0.9888) donc on peut dire que le modèle est bien ajusté.

### Test avec un niveau de signification 5%
On peut extraire le niveau de signification du summary de la manière suivante :

```{r}
significance_level = summary(kalama.lm)$coefficients[2,4]
print(significance_level)
```

Comme on peut le voir, ce niveau est bien inférieur à 0.05 donc on peut dire que la pente de la droite de régression est significativement différente de 0.

## Exercice 2

### Chargement des données
Nous avons créé un fichier `pnb_mortalite.txt` dans lequel nous avons copié-collé le contenu des données à importer. Cela nous a paru plus simple que de tout réécrire à la main dans une commande.

```{r}
pnb_data = read.table("data/pnb_mortalite.txt", header=TRUE)
pnb_data
```

### Résumé
```{r}
summary(pnb_data)
```

### Grapgique de nuage de points

```{r}
ggplot(pnb_data, aes(x = PNB, y = mortalite)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) + # Ajouter la droite de régression linéaire
geom_text_repel(aes(label = pays), size = 3, box.padding = 0.5) + # Ajouter les noms des pays avec ggrepel
labs(x = "Produit national brut", y = "Mortalité infantile", title = "Nuage de points : Mortalité infantile en fonction du PNB")
```

Comme on peut le voir sur le nuage de points ci-dessus, la plupart des pays suive la tendance sauf le Portugal qui sort très clairement du lot.

### Droite de régression linaire
```{r}
pnb_data.lm = lm(mortalite~PNB, data=pnb_data)
coef(pnb_data.lm)
summary(pnb_data.lm)
```

La pente est de **-0.16** et l'ordonnée à l'origine est de **51**.

### Coefficient $r$ et $R^2$

```{r}
cor(pnb_data$PNB, pnb_data$mortalite)
```

Le coefficient de corrélation `r` est de : **-0.78**. 

Le coefficient de *détermination*, indiqué dans le summary() ci-dessus, vaut 0.6106 et correspond effectivement au carré du coefficient de corrélation.

### Vérification des hypothèses

```{r}
resid_panel(pnb_data.lm, plots="all")
```

L'hypothèse que nous avions faite, soit que le Portugal est atypique, est confirmée par les graphiques ci-dessus. Que ce soit dans le Q-Q plot, le boxplot ou la distance de Cook, on voit qu'il y a une valeur atypique.

### Prédiction
```{r}
xnew <- matrix(c(100), nrow=1)
colnames(xnew) <- c("PNB")
xnew <- as.data.frame(xnew)
predict(pnb_data.lm, xnew, interval="pred")
```

La prédiction est de 35.12, soit exactement sur la ligne de la régression linéaire.

### Analyse supplémentaire

Ce nouveau modèle donne un meilleur coefficient de détermination (0.61 avec une régression linéaire, 0.78 avec un log).

Nous pensons qu'un modèle logarithmique serait effectivement mieux adapté à la prédiction de la mortalité infantile car un modèle linéaire implique qu'avec un certain PNB, le taux de mortalité infantile pourrait atteindre 0. En pratique, on pourra s'en approcher mais jamais atteindre le 0%.

## Exercice 3

### Graphique des corrélations

```{r}
litters = readRDS("data/litters.rds")

ggpairs(litters)
```

### Relation entre $lsize$ et $bodywt$
Nous pouvons constater qu'il existe une relation très forte entre les 2 variables (coef de corr : -0.95). Ce qui est plutot logique car elles représente la taille et le poids des individus.

### Relation avec $brainwt$
On observe une relation linéaire entre les différentes variable d'entrée et la variable de réponse.
elles sont de coef. de corr. -0.62 pour lsize et de 0.75 pour bodywt.

### Utilisation de $rgl$
```{r}
plotids <- with(litters, plot3d(lsize, bodywt, brainwt, type="s", col="blue"))

rglwidget(elementId = "plot3drgl")
```

Grâce à la 3d, nous pouvons encore mieux constater les relations linéaires

### Distinction des variables explicatives
Non, car les variables explicatives sont fortements corrélées. A cause de cela la distinction entre les deux sera difficile.

###  Modèle de régression linéaire multiple
```{r}
litters.lm = lm(brainwt~lsize+bodywt, data=litters)
coef(litters.lm)
```

yi = 0.178 + 0.0067 * lsize + 0.024 * bodywt

### Résumé et significativité des variables explicatives
```{r}
summary(litters.lm)
```

Comme on peut le voir, les niveaux sont bien inférieur à 0.05 donc on peut dire que la pente de la droite de régression est significativement différente de 0.

De plus nous pouvons constater une différence entre les deux variables explicative.
En effet, lsize obtient un résultat de 0.0475 alors que bodywt obtient 0.00228 se qui est meilleur.

### Graphique de nuage de points
```{r}
s3d <- scatterplot3d(litters$lsize, litters$bodywt, litters$brainwt, main = "",
  color="midnightblue", xlab="litter size", ylab="body weight",
  zlab="brain weight", angle = -60, pch=21, bg="orange")
  s3d$plane3d(litters.lm, draw_polygon=TRUE, lty.box="solid")
```

### Coefficient $R^2$ $R^2_{adj}$
Summary nous donne les résultats suivants : 

Multiple R-squared:  0.6505,    Adjusted R-squared:  0.6094

Nous pouvons en conclure que le modèle explique une proportion assez importante de la variabilité des données.

### Vérification des hypothèses
```{r}
resid_panel(litters.lm, plots="all")
```

Grâces aux plots, nous pouvons constater qu'il y a effectivement une correlation, mais qui est entachée par plusieurs valeurs atypiques (que l'on peut observer facilement sur le Cook'd D Plot)

Pour améliorer la qualité du model nous ponvons donc retirer ces valeurs atypiques.

## Exercice 4

### Graphique des corrélations

```{r}
wines = read_excel("data/Wine.xlsx")

ggpairs(wines)
```

Nous voyons 3 variable qui resortent pour expliquer la réponse :
- Aroma
- Body
- Flavor

### Modèle de régression linéaire multiple
```{r}
wines.lm = lm(Quality~Clarity+Aroma+Body+Flavor+Oakiness, data=wines)
coef(wines.lm)
```

yi = 3.99 + 2.34 * Clarity + 0.48 * Aroma + 0.27 * Body + 1.17 * Flavor - 0.68 * Oakiness

### Résumé et significativité des variables explicatives
```{r}
summary(wines.lm)
```

Les 3 variables avec un taux de signification supérieur à 5% sont Clarity, Aroma et Body avec respectivement 18%, 8% et 41%.

Si on rapporte ces résultats au graphique du point a), on constate qu'il s'agit également des variables ayant une corrélatio élevée avec la variable de réponse "Quality".

### Coefficient de détermination R^2

$R^2 = 0.7206$ 
$R^2_{adj} = 0.6769$

### e) Critères de comparaison des modèles

#### Critère d’Akaike (AIC)
Le critère d'information d'Akaika est un outil permettant d'équilibrer la complexité d'un modèle et sa capacité à bien représenter la réalité.
Il se calcule de la manière suivante :

$AIC = 2k - 2log(L)$

Plus ce critère est petit, plus le nombre de paramètres ($k$) actuel du modèle correspond à un bon compromis entre la qualité des prédictions ("vraisemblance", $L$ ci-dessus) et la complexité du modèle.

#### Critère d’Information Bayésien (BIC)
Le critère d'information Bayésien est très similaire à l'AIC sauf qu'il prend un autre paramètre en compte, la taille de l'échantillon de données.

Il se calcule de la manière suivante :
$BIC = k \cdot log(n) - 2log(L)$

Comme pour l'AIC, plus ce critère est petit, mieux c'est.

#### Critère $C_p$ de Mallows

Tout comme AIC ou BIC, le critère de Mallows permet de trouver un modèle qui ajuste bien les données tout en évitant une complexité excessive.

Il est calculé de la manière suivante :

$C_p = \frac{SSE_p}{\hat{\sigma}^2} - n + 2p$

Où
- $SSE_p$ est la somme des carrés des résidus
- $\hat{\sigma}^2$ est une estimation de la variance de l'erreur
- $n$ est la taille de l'échantillon
- $p$ est le nombre de paramètres du modèle

### f)

```{r}
choix <- regsubsets(Quality~., data=wines, nbest=1, nvmax=11)

plot(choix, scale="adjr2", col="midnightblue")
plot(choix, scale="bic", col="midnightblue")
plot(choix, scale="Cp", col="midnightblue")


leaps <- regsubsets(Quality~., data=wines, nbest=10)
summary(leaps)
```

Selon $R^2_{adj}$, il faut garder juste le paramètre flavor.

Selon BIC, il faut garder tous les paramètres.

Selon le critère de Mallows, il faut garder Aroma, Flavor et Oakiness.

On peut classer les critères en fonction du nombre de fois où ils ont été retenu par un des critères ci-dessus :
1. Flavor (3 fois)
2. Aroma et Oakiness (2 fois)
3. Clarity et Body (1 fois)

Nous avons donc décidé de garder les 3 paramètres Flavor, Aroma et Oakiness.

#### Création du modèle

```{r}
wines.new_lm = lm(Quality~Flavor+Aroma+Oakiness, data=wines)
coef(wines.new_lm)
summary(wines.new_lm)
resid_panel(wines.new_lm, plots="all")
```

On peut voir que la relation est linéaire (graphique Response vs Predicted), que la moyenne des erreurs semble nulle et a une variance assez constante (Residual Plot) et que les résidus sont répartis selon une distribution normale (Histogram).

#### Comparaison des modèles

```{r}
compare_performance()

# TODOOOOOOOOO
```